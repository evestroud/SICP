### 2.3.1 Quotation
* compound data using symbols
    * can look just like expressions
* in order to manipulate lists of symbols, need to be able to *quote* data
    * `(list a b)` produces a list of the values `a` and `b`, not the symbols themselves
    * quoting - marking symbols with a single quote to indicate to the interpreter that the symbol is itself data, and not to be evaluated
        * similar to quoting a phrase in natural language, e.g.
        * "say your name" vs "say 'your name' "
    * `(list 'a 'b)` produces a list of the symbols `a` and `b`
        * can also quote whole lists: `'(a b)` is equivalent to `(list 'a 'b)`
        * empty list is `'()`
* `(eq? s1 s2)`
    * compares equality of symbols
* `(memq list symbol)
    * returns sublist of `list` beginning with `symbol` if it is present, else returns false

2.3.2 Example: Symbolic Differentiation

### 2.3.3 Example: Representing Sets
* sets
    * collection of distinct objects
    * number of possible different representations
        * differ from each other significantly in several ways
* definition by data abstraction -- what are set operations?
    * union-set
        * takes two sets & returns set with all elements in either input
    * intersection-set
        * takes two sets & returns set with all elements that only appear in both inputs
    * element-of-set?
        * determines whether a given element is a member of a set
    * adjoin-set
        * takes an object and a set and returns a set containing all the members of the original set and the adjoined object

##### Sets as unordered lists
* can represent a set as a list in which no element appears more than once
    * `element-of-set? object set
        * implemented similar to `memq`
    * `adjoin-set object set
        * if `element-of-set? object set` return the original set
        * else cons the object to the original list
    * `intersection-set set1 set2
        * test if each item of set1 is in set2 with `element-of-set?`
        * cons all items where the predicate returns true
    * `union-set`
        * exercise 2.59
* efficiency
    * element-of-set? used in all other set operations
        * efficiency of other operation dependent on efficiency of element-of-set?
    * worst case theta(n) since must traverse the whole list if object is not in set
        * adjoin-set inherits this efficiency
        * intersection & union-set must traverse each item in set1 then call element-of-set? on list2 each time, resulting in theta(n^2) efficiency

Exercise 2.60
* efficiency - d is number of duplicate values
    * element-of-set?
        * theta(n + d) 
    * intersection-set
        * theta(n^2 + d^2)
    * union-set & adjoin-set
        * theta(1)
* this implementation would be more efficient in use cases where there are not likely to be large amounts of duplicates or calls to union-set or intersection-set (both can produce duplicates)

##### Sets as ordered lists
* one way to speed up set operations
    * requires a way of comparing objects with > < =
* operations
    * element-of-set? object set
        * no longer have to scan entire set, only until an item >= to object is found
        * worst case (object >= largest item in set)
        * average case n / 2
            * still O(N) but better than before
        * adjoin-set follows a similar pattern
    * intersection-set set1 set2
        * "dual pointer" method (except traversing LLs)
        * compare the cars of each set:
            * if equal, add to intersection
            * if (car set1) > (car set2), advance set2
            * if (car set1) < (car set2), advance set1
            * if either is empty, return null
        * O(N) time complexity where N is the combined length of the two sets
        * union-set follows a similar pattern

##### Sets as binary trees
* ordered list in a tree of nodes
    * **BST**
        * basic principles apply
    * in a balanced tree, comparisons always halve the size of the remaining search tree
    * O(log N) time for most operations
* can be represented with lists
    * each node is a list of three items:
        * value, left subtree, right subtree

Exercise 2.63
1. They both produce an in-order traversal of the tree. `tree->list-2` technically traverses in reverse order but uses `cons` to build an in-order list. Both produce `(1 3 5 7 9 11)` for the example trees.
2. Both produce O(log N) recursive calls, but `tree->list-1` will traverse the whole flattened left side of the list in each frame at the call to `append`. I suspect this will produce a time complexity in the order of O(N log N)

Exercise 2.64
1. 
> It recursively builds a tree from a list of elements, and an integer n which starts out equal to the length of the list. N is progressively halfed (left-size) at each recursive call (partial-tree elts left-size) until reaching 0. Upon hitting 0, a list is made of an empty list and the list of remaining elements. Upon hitting this return and percolating back up, the tree and list (left-result) are separated into the tree and list sections (left-tree and non-left-elts), the first of the remaining elements is identified as the val at this level of the tree (this-entry). The right side of the tree at the node immediately above the empty list is then built by a recursive call, but this time looking at the number of elements remaining after the previous recursive call's halved n (right-size) and the elements remaining from that call (non-left-elts). The results of this second recursive call (right-result) consists of the right branch of the tree for this level of recursion and any remaining elements (remaining-elts) which will be processed in the frame above. The final return creates a list of a tree with this-entry left-tree, and right- tree, and the list of any elements that are still remaining.
```
(5
  (1
    (3))
  (9
    (7)
    (11)))
``` 
2. 
> Overall, this function builds a tree out of the elements by index, by recursively building up a tree of N elements out of trees of n/2-1 elements. Its stack frames "traverse" the tree it is building in an in-order traversal. It operates in N time and takes log N additional space. It performs no sorting on the list elements as it makes the tree, but builds a tree "sorted by" list index.

##### Sets and information retrieval
* examining different representations of sets shows how representation of a data object can have a large impact on performance
    * these techniques of data abstraction and analysis are used throughout software engineering
    * sets specifically chosen to demonstrate applications in information retrieval
* databases
    * typical data management system spends a large amount of time accessing & modifying data in records
        * requires an efficient mechanism for accessing records
    * key - part of a database record that serves as an identifier 
        * e.g. ID number
        * databases search for keys similar to how set representation examples search for the given entry
    * typically tree-based search structure
    * methodology of data abstraction very relevant
        * initial implementation can be simple and straightforwards, then updated to be more sophisticated while maintaining the same API


### 2.3.4 Example: Huffman Encoding Trees
* representing data in binary format
    * 7 bits allows 2^7 = 128 possible characters encoded
    * in general, if needing to distinguish *n* different symbols, need log2 *n* bits per symbol
    * "fixed length codes"
* variable length codes
    * morse code does not use same number of dots and dashes for each character
        * most common character 'e' is represented by a single dot
    * in general, can encode data more efficiently by assigning shorter codes to more frequent symbols
    * one difficulty is knowing when one symbol stops and another starts
        * morse uses pauses to distinguish symbols
        * another solution is to design the code so no symbol for any code is the beginning of another code
            * e.g. if 0 represents a character, no other codes can start with 0
* TODO finish this section

Exercise 2.67
> (A D A B B C A)

Exercise 2.70
> 84 bits are required for the Huffman encoding. An eight-symbol alphabet would require a 4 bit fixed-length encoding, so the 36 symbol long song would require 144 bits to encode.

Exercise 2.71
(in notebook)
> The most frequent symbol in this kind of structure always requires one bit to encode. The least frequent requires n - 1 bits to encode.

Exercise 2.72
> In the worst case, determining which branch of the tree the symbol is in requires time proportional to the number of leaves in the tree at each layer. In the case of the symbol not being present in the tree, and in the case of the most frequently used symbol, this will be a total of O(N) time.
> The number of recursive calls made should be roughly logarithmic base 2 in relation to the number of leaves in the tree. However, this depends on how well balanced the tree is. In the worst case (weights have an exponential pattern of growth, as in 2.71) the number of recursive calls will be proportional to the number of leaves. While the number of symbols to be searched decreases at each level, in this worst case structure it only decreases by one so the order of growth of successive recursive calls in this structure is a sum of natural numbers (exponential function).
> Overall the average performance of this algorithm is O(N log N). The best case is O(N) and worst case is O(N^2).